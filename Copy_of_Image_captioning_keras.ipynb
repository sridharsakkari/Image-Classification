{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Image_captioning_keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1QsCbJ7zrcuDgi7DOAMruF0Sl1BEkoEVQ",
      "authorship_tag": "ABX9TyOhah6gRkaupaYCQqtzdzST",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "819f7a785a404125b7ea364804f45187": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_00a0e079e2eb4084b93d35b006e63d41",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e6a74bb61a2b4236adfe43659c0a96e6",
              "IPY_MODEL_4d27b18c6f474d4095a89321c3ab6b92"
            ]
          }
        },
        "00a0e079e2eb4084b93d35b006e63d41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e6a74bb61a2b4236adfe43659c0a96e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_db2e6a1eacb6406e9691d2c2dfe36721",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_657f7342eddf4435a5287fb097185fb4"
          }
        },
        "4d27b18c6f474d4095a89321c3ab6b92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bd07d821792d4092b6a4f6b1b0c765dc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/? [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2ec4150314974e6ab970552ec86cca9c"
          }
        },
        "db2e6a1eacb6406e9691d2c2dfe36721": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "657f7342eddf4435a5287fb097185fb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bd07d821792d4092b6a4f6b1b0c765dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2ec4150314974e6ab970552ec86cca9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sridharsakkari/sree/blob/master/Copy_of_Image_captioning_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vezVSoiT2gyt",
        "colab_type": "text"
      },
      "source": [
        "Develop a model to determine the image captioning by using pretrained VGG16 model and CNN-RNN Architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v07RT7P09TB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "from zipfile import ZipFile\n",
        "\n",
        "zipped_imgfile = ZipFile('/content/drive/My Drive/Image_captioning/Flickr8k_Dataset.zip','r')\n",
        "zipped_imgfile.extractall()\n",
        "\n",
        "zipped_captfile = ZipFile('/content/drive/My Drive/Image_captioning/Flickr8k_text.zip','r')\n",
        "zipped_captfile.extractall()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYP5JoGbB1rm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "819f7a785a404125b7ea364804f45187",
            "00a0e079e2eb4084b93d35b006e63d41",
            "e6a74bb61a2b4236adfe43659c0a96e6",
            "4d27b18c6f474d4095a89321c3ab6b92",
            "db2e6a1eacb6406e9691d2c2dfe36721",
            "657f7342eddf4435a5287fb097185fb4",
            "bd07d821792d4092b6a4f6b1b0c765dc",
            "2ec4150314974e6ab970552ec86cca9c"
          ]
        },
        "outputId": "e4f3a8b6-b86a-417f-8832-9344ebed3084"
      },
      "source": [
        "import string\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import os\n",
        "from pickle import dump,load\n",
        "\n",
        "from keras.applications.xception import Xception, preprocess_input\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers.merge import add\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
        "\n",
        "\n",
        "# small library for seeing the progress of loops.\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "tqdm().pandas()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "819f7a785a404125b7ea364804f45187",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3-o_VKkCe2V",
        "colab_type": "text"
      },
      "source": [
        "From the text folder we have imported, all image captions are stored in the file Flickr8k.token.txt. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67NkgGTcBh0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading a file \n",
        "\n",
        "def load_file(filename):\n",
        "\n",
        "  file = open(filename,'r')\n",
        "  text = file.read()\n",
        "  file.close()\n",
        "\n",
        "  return text\n",
        "\n",
        "\n",
        "# get all images with their captions \n",
        "\n",
        "def image_captions(filename):\n",
        "\n",
        "  file = load_file(filename)\n",
        "  captions = file.split('\\n') # as every caption is in new line\n",
        "\n",
        "  descriptions = []\n",
        "  images = []\n",
        "\n",
        "  for caption in captions[:-1]: #last value in captions file is ''\n",
        "    \n",
        "    img,capt = caption.split('\\t')\n",
        "\n",
        "    descriptions.append(capt)\n",
        "    images.append(img[:-2])\n",
        "\n",
        "  return images,descriptions"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gab9_kgKu9l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = load_file('Flickr8k.token.txt')\n",
        "images,descriptions = image_captions('Flickr8k.token.txt')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkcxY3PHOllA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ae08059d-4105-4117-9abc-7943c8972f24"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz31RGdCP6uR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKjXTnUiTRzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os import listdir\n",
        "from pickle import dump\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.models import Model\n",
        "from keras.utils import plot_model"
      ],
      "execution_count": 347,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7EtNFcbZVFQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def feature_extraction_from_images(Image_dir):\n",
        "\n",
        "  model = VGG16()\n",
        "\n",
        "  model.layers.pop()\n",
        "\n",
        "  model = Model(inputs = model.inputs,outputs = model.layers[-1].output)\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  # extract features from each image\n",
        "\n",
        "  features = dict()\n",
        "  Image_path = os.listdir(Image_dir) \n",
        "  for name in Image_path:\n",
        "\n",
        "    image = load_img(Image_dir+name,target_size=(224,224))\n",
        "\n",
        "    image = img_to_array(image)\n",
        "\n",
        "    image = image.reshape((1,image.shape[0],image.shape[1],image.shape[2]))\n",
        "\n",
        "    image = preprocess_input(image)\n",
        "\n",
        "    feature = model.predict(image,verbose=0)\n",
        "\n",
        "    image_id = name.split('.')[0]\n",
        "\n",
        "    features[image_id] = feature\n",
        "\n",
        "  return features\n",
        "\n",
        "\n",
        "   \n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b59mIAula3eP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Image_dir = ('/content/Flicker8k_Dataset/')\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Te0WILNbbRy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        },
        "outputId": "84e8c8c4-26b6-4377-f119-b54ef92a893d"
      },
      "source": [
        "features = feature_extraction_from_images(Image_dir)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "=================================================================\n",
            "Total params: 134,260,544\n",
            "Trainable params: 134,260,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXbgIwYqf7wu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dump(features,open('features.pkl', 'wb'))  #dump is used to create pickle file and save features into it\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5jcs4L90fUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT_aYB4NcyX_",
        "colab_type": "text"
      },
      "source": [
        "At first, the text data is prepared by loading the files from the root directory and perform cleaning of the vocabulary data.\n",
        "\n",
        "step 1 : Load the raw file consists of tokens by using the load_doc function. \n",
        "\n",
        "step 2 : The loaded document is passed through load_descriptions function where tha data is split into image identifier and description.  Every identifier has multiple probable captions. \n",
        "\n",
        "step 3 : Now the descriptions are cleaned word to word by using some tokenization techniques.. by function clean_descriptions\n",
        "\n",
        "step 4 : convert the cleaned list into a vocabulary of words by to_vocabulary function.\n",
        "\n",
        "step 5 : save the file to a descriptions.txt file as a vocab dictionary\n",
        "\n",
        "\n",
        "step 6 : Load the traindataset, valid dataset and test dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYqtw541fhIA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b0b8bbce-a822-45e2-df37-86ff953f545c"
      },
      "source": [
        "text = load_doc('Flickr8k.token.txt')\n",
        "text[:166]"
      ],
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1000268201_693b08cb0e.jpg#0\\tA child in a pink dress is climbing up a set of stairs in an entry way .\\n1000268201_693b08cb0e.jpg#1\\tA girl going into a wooden building .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 290
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tdyzcgzcx1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mapping = load_descriptions(text)"
      ],
      "execution_count": 262,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm-ygSIfeGuJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "31597020-d766-46e7-f7a9-f3b1b258cd18"
      },
      "source": [
        "list(mapping.items())[1]"
      ],
      "execution_count": 279,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('1001773457_577c3a7d70',\n",
              " ['A black dog and a spotted dog are fighting',\n",
              "  'A black dog and a tri-colored dog playing with each other on the road .',\n",
              "  'A black dog and a white dog with brown spots are staring at each other in the street .',\n",
              "  'Two dogs of different breeds looking at each other on the road .',\n",
              "  'Two dogs on pavement moving toward each other .'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 279
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtYw9fUH0JDW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bb977f49-7fde-4dfc-ed37-bedc5cd57d6b"
      },
      "source": [
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "\n",
        "\n",
        "# extract descriptions for images\n",
        "def load_descriptions(doc):\n",
        "\tmapping = dict()\n",
        "\t# process lines\n",
        "\tfor line in doc.split('\\n'):   #split the data as per linewise i.e. '\\n'\n",
        "\t\t\n",
        "\t\ttokens = line.split()         # split line by white space\n",
        "\t\tif len(line) < 2:\n",
        "\t\t\tcontinue\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]    # take the first token as the image id, the rest as the description\n",
        "\n",
        "\t\timage_id = image_id.split('.')[0]       # remove filename from image id i.e. file format is removed from the identifier\n",
        "\n",
        "\t\timage_desc = ' '.join(image_desc)       # convert description tokens back to string\n",
        "\n",
        "\t\t# create the list if needed\n",
        "\t\tif image_id not in mapping:\n",
        "\t\t\tmapping[image_id] = list()\n",
        "\t\t# store description\n",
        "\t\tmapping[image_id].append(image_desc)\n",
        "  \n",
        "\n",
        "\treturn mapping\n",
        "\n",
        "\n",
        "\n",
        "def clean_descriptions(descriptions):\n",
        "\t\n",
        "\ttable = str.maketrans('', '', string.punctuation)      # prepare translation table for removing punctuation\n",
        "\n",
        "\tfor key, desc_list in descriptions.items():            # descriptions is a dictionary with image identifiers mapping with captions..\n",
        "                                                         # key : identifiers, desc_list : captions ....(1,2,...)\n",
        "   \n",
        "\t\tfor i in range(len(desc_list)):                      # len of captions... i.e. numnber of captions in the list        \n",
        "    \n",
        "\t\t\tdesc = desc_list[i]                                # extract one element\n",
        "\n",
        "\t\t\t# tokenize\n",
        "\n",
        "\t\t\tdesc = desc.split()                                # split the caption according to whitespaces\n",
        "\t\t\t\n",
        "\t\t\tdesc = [word.lower() for word in desc]             # convert to lower case\n",
        "\t\t\t\n",
        "\t\t\tdesc = [w.translate(table) for w in desc]          # remove punctuation from each token\n",
        "\t\t\t\n",
        "\t\t\tdesc = [word for word in desc if len(word)>1]      # remove hanging 's' and 'a'\n",
        "\t\t\t\n",
        "\t\t\tdesc = [word for word in desc if word.isalpha()]   # remove tokens with numbers in them\n",
        "\t\t\t\n",
        "\t\t\tdesc_list[i] =  ' '.join(desc)                     # store as string\n",
        "\n",
        "\n",
        "\n",
        "# convert the loaded descriptions into a vocabulary of words\n",
        "def to_vocabulary(descriptions):\n",
        "\t# build a list of all description strings\n",
        "\n",
        "\tall_desc = set()\n",
        " \n",
        "\tfor key in descriptions.keys():\n",
        "   \n",
        "\t\t[all_desc.update(d.split()) for d in descriptions[key]]         \n",
        "\treturn all_desc\n",
        "\n",
        "\n",
        "\n",
        "# save descriptions to file, one per line\n",
        "def save_descriptions(descriptions, filename):             # save the vocab list to a file descriptions.txt\n",
        "\tlines = list()\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\tfor desc in desc_list:\n",
        "\t\t\tlines.append(key + ' ' + desc)\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()\n",
        "\n",
        "filename = 'Flickr8k.token.txt'\n",
        "# load descriptions\n",
        "doc = load_doc(filename)\n",
        "# parse descriptions\n",
        "descriptions = load_descriptions(doc)\n",
        "print('Loaded: %d ' % len(descriptions))\n",
        "# clean descriptions\n",
        "clean_descriptions(descriptions)\n",
        "# summarize vocabulary\n",
        "vocabulary = to_vocabulary(descriptions)\n",
        "print('Vocabulary Size: %d' % len(vocabulary))\n",
        "# save to file\n",
        "save_descriptions(descriptions, 'descriptions.txt')"
      ],
      "execution_count": 318,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded: 8092 \n",
            "Vocabulary Size: 8763\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkbFz82a1R-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading data train,valid,test data \n",
        "\n",
        "\n",
        "def load_set(filename):\n",
        "\tdoc = load_doc(filename)\n",
        "\tdataset = list()\n",
        "\t# process line by line\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# skip empty lines\n",
        "\t\tif len(line) < 1:\n",
        "\t\t\tcontinue\n",
        "\t\t# get the image identifier\n",
        "\t\tidentifier = line.split('.')[0]\n",
        "\t\tdataset.append(identifier)\n",
        "\treturn set(dataset)"
      ],
      "execution_count": 319,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwECziiI4HQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load photos from train /test dataset and below function will load clean descriptions\n",
        "\n",
        "def load_clean_descriptions(filename, dataset):\n",
        "\n",
        "  doc = load_doc(filename)\n",
        "\n",
        "  descriptions = {}\n",
        "\n",
        "  for line in doc.split('\\n'):\n",
        "\n",
        "    if len(line)>2:\n",
        "    \n",
        "     tokens = line.split()\n",
        "     \n",
        "     image_id,image_desc = tokens[0], tokens[1:]\n",
        "\n",
        "    if image_id in dataset:\n",
        "\n",
        "      \n",
        "\n",
        "      if image_id not in descriptions:\n",
        "\n",
        "        descriptions[image_id] = list()\n",
        "\n",
        "      desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
        "\n",
        "      descriptions[image_id].append(desc)\n",
        "\n",
        "  return descriptions"
      ],
      "execution_count": 320,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxqZsDED5yzZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = load_set('Flickr_8k.trainImages.txt')\n",
        "descriptions = load_clean_descriptions('descriptions.txt',dataset)"
      ],
      "execution_count": 321,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9R-AzlN8N_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def photo_features(filename,dataset):\n",
        "  all_photo_features = load(open(filename,'rb'))\n",
        "  features = {k:all_photo_features[k] for k in dataset}\n",
        "\n",
        "  return features"
      ],
      "execution_count": 322,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phb6QAPsbtuo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "photofeatures = photo_features('features.pkl',dataset)"
      ],
      "execution_count": 323,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4sjPhZmqeiN",
        "colab_type": "text"
      },
      "source": [
        "Before feeding the processed data to the model, the data shall be encoded to numbers.\n",
        "Create a consistent mapping from words to unique integer values\n",
        "\n",
        "In keras, we have a tokenizer class to perform this function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofXcwWurcAEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_lines(descriptions):      # convert a cleaned dictionary of tokens to list\n",
        "\n",
        "  all_desc = list()\n",
        "\n",
        "  for key in descriptions.keys():\n",
        "    [all_desc.append(d) for d in descriptions[key]]\n",
        "\n",
        "  return all_desc\n"
      ],
      "execution_count": 324,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiSGHG5EsfoO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fit a tokenizer \n",
        "\n",
        "def create_tokenizer(descriptions):\n",
        "  lines_ = to_lines(descriptions)\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines_)\n",
        "\n",
        "  return tokenizer"
      ],
      "execution_count": 329,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frweaOkXtJS9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = create_tokenizer(descriptions)"
      ],
      "execution_count": 330,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lni1vxR_tQ52",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) +1"
      ],
      "execution_count": 335,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZyLdzyPwTAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_length(descriptions):\n",
        "\n",
        "  lines_ = to_lines(descriptions)\n",
        "\n",
        "  return max(len(d.split()) for d in lines_)"
      ],
      "execution_count": 342,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qa4U8S_Rukli",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_sequences(tokenizer,max_length,descriptions,photos,vocab_size):\n",
        "\n",
        "  X1,X2,y = list(),list(),list()\n",
        "\n",
        "  for key,desc_list in descriptions.items():\n",
        "\n",
        "    for desc in desc_list:\n",
        "\n",
        "      seq = tokenizer.texts_to_sequences([desc])[0]\n",
        "\n",
        "      for i in range(1,len(seq)):\n",
        "\n",
        "        in_seq,out_seq = seq[:i],seq[i]\n",
        "\n",
        "        in_seq = pad_sequences([in_seq],maxlen =max_length)[0]\n",
        "\n",
        "        out_seq = to_categorical([out_seq],num_classes=vocab_size)[0]\n",
        "\n",
        "        X1.append(photos[key][0])\n",
        "        X2.append(in_seq)\n",
        "        y.append(out_seq)\n",
        "\n",
        "  return array(X1),array(X2),array(y)\n"
      ],
      "execution_count": 337,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QDMP1xVxfbQ",
        "colab_type": "text"
      },
      "source": [
        "Step 1 : Photo Feature Extractor : images are processed through pretrained vgg16 model without output layer .\n",
        "\n",
        "Step 2 : Word embedding layer followed by LSTM layer  (sequence processor)\n",
        "\n",
        "Step 3 : Decoder: both a feature extractor and sequence processor output a fixed length vector. \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A31j9Np7yMHw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9bad7b36-ea95-4c1e-a9cf-4e5e83f9c845"
      },
      "source": [
        "max_length(descriptions)"
      ],
      "execution_count": 343,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 343
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4VRDn3iyxT4",
        "colab_type": "text"
      },
      "source": [
        "The sequence processor produces a word embedding layer of length 34 words.\n",
        "The photo extractor produces an output of size 4096.\n",
        "this is processed through a dense layer to produce a 256 sized vector\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjLpsnmQywQj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model2(vocab_size,max_length):\n",
        "\n",
        "  # feature extractor\n",
        "  inputs1 = Input(shape = (4096,))\n",
        "  fe1 = Dropout(0.5)(inputs1)\n",
        "  fe2 = Dense(256,activation='relu')(fe1)\n",
        "\n",
        "  # sequence processor \n",
        "\n",
        "  inputs2 = Input(shape=(max_length,))\n",
        "  se1 = Embedding(vocab_size,256,mask_zero =True)(inputs2)\n",
        "  se2 = Dropout(0.5)(se1)\n",
        "  se3 = LSTM(256)(se2)\n",
        "\n",
        "\n",
        "  # decoder model\n",
        "\n",
        "  decoder1 = add([fe2,se3])\n",
        "  decoder2 = Dense(256,activation='relu')(decoder1)\n",
        "  outputs = Dense(vocab_size,activation='softmax')(decoder2)\n",
        "\n",
        "  model = Model(inputs = [inputs1,inputs2],outputs = outputs)\n",
        "  model.compile(loss = 'categorical_crossentropy',optimizer = 'adam')\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  plot_model(model, to_file='model.png', show_shapes=True)\n",
        "\t\n",
        "  return model\n",
        "\n",
        "\n"
      ],
      "execution_count": 348,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK_XZ8pd1qfs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "c7fb6afc-ee56-4601-e070-a26551734a75"
      },
      "source": [
        "model2(vocab_size,34)"
      ],
      "execution_count": 350,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_12 (InputLayer)           (None, 34)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_11 (InputLayer)           (None, 4096)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 34, 256)      1940224     input_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 4096)         0           input_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 34, 256)      0           embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 256)          1048832     dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   (None, 256)          525312      dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 256)          0           dense_7[0][0]                    \n",
            "                                                                 lstm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 256)          65792       add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 7579)         1947803     dense_8[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 5,527,963\n",
            "Trainable params: 5,527,963\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.training.Model at 0x7f602ff2dcf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 350
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cBj0Es22fjQ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wphXztk_2BVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}